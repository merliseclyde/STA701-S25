[
  {
    "objectID": "abstracts/braden_scherting.html",
    "href": "abstracts/braden_scherting.html",
    "title": "“So many bugs… Decoupling NMF”",
    "section": "",
    "text": "[Early stage work towards:] Nonnegative matrix factorization (NMF) is a well-established method for flexibly modeling high-dimensional, sparse count data that admits interpretable decompositions of latent constructs when the estimates are sparse. Despite the flexibility of NMF, error distributions implicit to deterministic solutions and popular, convenient hierarchical extensions falter when simultaneously confronted with extreme sparsity and large counts. Thus, misspecified models require larger factorization ranks to adequately fit such data, further complicating the choice of rank. Moreover, estimates produced by flexible hierarchical models are typically only approximately sparse. Motivated by a global-scale study of arthropod abundance, we present a decoupled, two-stage approach to NMF. In the first stage, abundances are modeled hierarchically; the chosen prior shrinks loadings and unnecessary factors while allowing overdispersion levels to vary among factors. A second-stage loss minimization procedure sparsifies estimates, which allow us to derive meta profiles of the ~50000 study species. Inference via MCMC is rapid thanks to the observed sparsity, and the second-stage optimization leverages existing efficient NMF computational schemes.\n\n\nDavid Dunson\n\n\n\n3rd year"
  },
  {
    "objectID": "abstracts/braden_scherting.html#abstract",
    "href": "abstracts/braden_scherting.html#abstract",
    "title": "“So many bugs… Decoupling NMF”",
    "section": "",
    "text": "[Early stage work towards:] Nonnegative matrix factorization (NMF) is a well-established method for flexibly modeling high-dimensional, sparse count data that admits interpretable decompositions of latent constructs when the estimates are sparse. Despite the flexibility of NMF, error distributions implicit to deterministic solutions and popular, convenient hierarchical extensions falter when simultaneously confronted with extreme sparsity and large counts. Thus, misspecified models require larger factorization ranks to adequately fit such data, further complicating the choice of rank. Moreover, estimates produced by flexible hierarchical models are typically only approximately sparse. Motivated by a global-scale study of arthropod abundance, we present a decoupled, two-stage approach to NMF. In the first stage, abundances are modeled hierarchically; the chosen prior shrinks loadings and unnecessary factors while allowing overdispersion levels to vary among factors. A second-stage loss minimization procedure sparsifies estimates, which allow us to derive meta profiles of the ~50000 study species. Inference via MCMC is rapid thanks to the observed sparsity, and the second-stage optimization leverages existing efficient NMF computational schemes.\n\n\nDavid Dunson\n\n\n\n3rd year"
  },
  {
    "objectID": "abstracts/benedetta-bruni.html",
    "href": "abstracts/benedetta-bruni.html",
    "title": "A partial likelihood approach to tree-based density modeling and its application in Bayesian inference",
    "section": "",
    "text": "Tree-based priors for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Thus, existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning in Bayesian inference, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox’s partial likelihood. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and, in particular, to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from adopting the partial likelihood.\n\n\nLi Ma\n\n\n\n3rd year PhD student in the Department of Statistics."
  },
  {
    "objectID": "abstracts/benedetta-bruni.html#abstract",
    "href": "abstracts/benedetta-bruni.html#abstract",
    "title": "A partial likelihood approach to tree-based density modeling and its application in Bayesian inference",
    "section": "",
    "text": "Tree-based priors for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Thus, existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning in Bayesian inference, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox’s partial likelihood. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and, in particular, to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from adopting the partial likelihood.\n\n\nLi Ma\n\n\n\n3rd year PhD student in the Department of Statistics."
  },
  {
    "objectID": "abstracts/luke_vrotsos.html",
    "href": "abstracts/luke_vrotsos.html",
    "title": "Dynamic graphical models: Theory, structure and counterfactual forecasting",
    "section": "",
    "text": "Simultaneous graphical dynamic linear models (SGDLMs) provide advances in flexibility, parsimony and scalability of multivariate time series analysis, with proven utility in forecasting. Core theoretical aspects of such models are developed, including new results linking dynamic graphical and latent factor models. Methodological developments extend existing Bayesian sequential analyses for counterfactual forecasting. The latter, involving new Bayesian computational developments for missing data in SGDLMs, is motivated by causal applications. A detailed example illustrating the models and new methodology concerns global macroeconomic time series with complex, time-varying cross-series relationships and primary interests in potential causal effects.\nSlides\n\n\nMike West\n\n\n\nLuke is a third-year PhD student in the Department of Statistical Science working with Mike West on Bayesian time series models."
  },
  {
    "objectID": "abstracts/luke_vrotsos.html#abstract",
    "href": "abstracts/luke_vrotsos.html#abstract",
    "title": "Dynamic graphical models: Theory, structure and counterfactual forecasting",
    "section": "",
    "text": "Simultaneous graphical dynamic linear models (SGDLMs) provide advances in flexibility, parsimony and scalability of multivariate time series analysis, with proven utility in forecasting. Core theoretical aspects of such models are developed, including new results linking dynamic graphical and latent factor models. Methodological developments extend existing Bayesian sequential analyses for counterfactual forecasting. The latter, involving new Bayesian computational developments for missing data in SGDLMs, is motivated by causal applications. A detailed example illustrating the models and new methodology concerns global macroeconomic time series with complex, time-varying cross-series relationships and primary interests in potential causal effects.\nSlides\n\n\nMike West\n\n\n\nLuke is a third-year PhD student in the Department of Statistical Science working with Mike West on Bayesian time series models."
  },
  {
    "objectID": "abstracts/bongjung_sung.html",
    "href": "abstracts/bongjung_sung.html",
    "title": "Testing Separability of Covariance Matrices in High-Dimensional Settings",
    "section": "",
    "text": "Due to their parsimony, separable covariance models have been popular in modeling matrix-variate data. Yet, they may yield misleading inferences if the separability assumption is incorrect. Likelihood ratio tests have tractable null distributions and good power when the sample size n is at least the number of variables p, but are not well-defined otherwise. Other existing separability tests for the p&gt;n case have low power for small sample sizes and null distributions dependent on unknown parameters, preventing exact error rate control. To address these issues, we propose novel invariant tests using the core covariance matrix, a complementary notion to a separable covariance matrix. We show that testing separability of a covariance matrix is equivalent to testing sphericity of its core component. Based on this, we construct the test statistics that are well-defined in high-dimensional settings and have distributions that are invariant under the null of separability, allowing exact simulation of null distributions. We study asymptotic null distributions and show consistency of our tests when p/n→ϒ∈(0,∞). The large power of our proposed tests compared to existing procedures is also numerically illustrated.\n\n\nPeter Hoff\n\n\n\nBongjung is a third year PhD student interested in the statistical application with a core covariance matrix."
  },
  {
    "objectID": "abstracts/bongjung_sung.html#abstract",
    "href": "abstracts/bongjung_sung.html#abstract",
    "title": "Testing Separability of Covariance Matrices in High-Dimensional Settings",
    "section": "",
    "text": "Due to their parsimony, separable covariance models have been popular in modeling matrix-variate data. Yet, they may yield misleading inferences if the separability assumption is incorrect. Likelihood ratio tests have tractable null distributions and good power when the sample size n is at least the number of variables p, but are not well-defined otherwise. Other existing separability tests for the p&gt;n case have low power for small sample sizes and null distributions dependent on unknown parameters, preventing exact error rate control. To address these issues, we propose novel invariant tests using the core covariance matrix, a complementary notion to a separable covariance matrix. We show that testing separability of a covariance matrix is equivalent to testing sphericity of its core component. Based on this, we construct the test statistics that are well-defined in high-dimensional settings and have distributions that are invariant under the null of separability, allowing exact simulation of null distributions. We study asymptotic null distributions and show consistency of our tests when p/n→ϒ∈(0,∞). The large power of our proposed tests compared to existing procedures is also numerically illustrated.\n\n\nPeter Hoff\n\n\n\nBongjung is a third year PhD student interested in the statistical application with a core covariance matrix."
  },
  {
    "objectID": "abstracts/lorenzo_mauri.html",
    "href": "abstracts/lorenzo_mauri.html",
    "title": "Spectral decomposition-assisted multi-study factor analysis",
    "section": "",
    "text": "This work focuses on covariance estimation for multi-study data. Popular approaches employ factor-analytic terms with shared and study-specific loadings that decompose the variance into (i) a shared low-rank component, (ii) study-specific low-rank components, and (iii) a diagonal term capturing idiosyncratic variability. Our proposed methodology estimates the latent factors via spectral decompositions and infers the factor loadings via surrogate regression tasks, avoiding identifiability and computational issues of existing alternatives. The approximation error decreases as the sample size and the data dimension diverge, formalizing a blessing of dimensionality. Conditionally on the factors, loadings and residual error variances are inferred via conjugate normal-inverse gamma priors. The conditional posterior distribution of factor loadings has a simple product form across outcomes, facilitating parallelization. We show favorable asymptotic properties, including central limit theorems for point estimators and posterior contraction. The methods are applied to integrate three studies on gene associations among immune cells.\n\n\nDavid B. Dunson\n\n\n\nLorenzo is a third year student working on latent factor models."
  },
  {
    "objectID": "abstracts/lorenzo_mauri.html#abstract",
    "href": "abstracts/lorenzo_mauri.html#abstract",
    "title": "Spectral decomposition-assisted multi-study factor analysis",
    "section": "",
    "text": "This work focuses on covariance estimation for multi-study data. Popular approaches employ factor-analytic terms with shared and study-specific loadings that decompose the variance into (i) a shared low-rank component, (ii) study-specific low-rank components, and (iii) a diagonal term capturing idiosyncratic variability. Our proposed methodology estimates the latent factors via spectral decompositions and infers the factor loadings via surrogate regression tasks, avoiding identifiability and computational issues of existing alternatives. The approximation error decreases as the sample size and the data dimension diverge, formalizing a blessing of dimensionality. Conditionally on the factors, loadings and residual error variances are inferred via conjugate normal-inverse gamma priors. The conditional posterior distribution of factor loadings has a simple product form across outcomes, facilitating parallelization. We show favorable asymptotic properties, including central limit theorems for point estimators and posterior contraction. The methods are applied to integrate three studies on gene associations among immune cells.\n\n\nDavid B. Dunson\n\n\n\nLorenzo is a third year student working on latent factor models."
  },
  {
    "objectID": "abstracts/cathy_lee.html",
    "href": "abstracts/cathy_lee.html",
    "title": "Sharp Bounding Null Effects in Causal Experiments with Ordinal Outcomes",
    "section": "",
    "text": "Ordinal outcomes are commonly measured across disciplines. However, with such ordered categorical outcomes, we do not have information on the magnitude of the difference between outcomes. Because of this, commonly studied estimands in causal experiments, such as the average treatment effect (ATE), are not well defined. One approach to dealing with ordinal outcomes is latent variable modeling. However, a partial identification strategy may be desirable as it does not require making modeling assumptions. To that end, we prove sharp upper and lower bounds on the probability that the potential outcome under treatment is equal to that under control. The infrequency at which the sharp lower bound is strictly greater than zero motivates us to prove how the sharp lower bound changes when we inject belief on the magnitude of the probability that the outcome under the two groups are equal.\n\n\nAlex Volfovsky\n\n\n\nCathy is a 4th year whose research focus is in causal inference."
  },
  {
    "objectID": "abstracts/cathy_lee.html#abstract",
    "href": "abstracts/cathy_lee.html#abstract",
    "title": "Sharp Bounding Null Effects in Causal Experiments with Ordinal Outcomes",
    "section": "",
    "text": "Ordinal outcomes are commonly measured across disciplines. However, with such ordered categorical outcomes, we do not have information on the magnitude of the difference between outcomes. Because of this, commonly studied estimands in causal experiments, such as the average treatment effect (ATE), are not well defined. One approach to dealing with ordinal outcomes is latent variable modeling. However, a partial identification strategy may be desirable as it does not require making modeling assumptions. To that end, we prove sharp upper and lower bounds on the probability that the potential outcome under treatment is equal to that under control. The infrequency at which the sharp lower bound is strictly greater than zero motivates us to prove how the sharp lower bound changes when we inject belief on the magnitude of the probability that the outcome under the two groups are equal.\n\n\nAlex Volfovsky\n\n\n\nCathy is a 4th year whose research focus is in causal inference."
  },
  {
    "objectID": "abstracts/piotr-suder.html",
    "href": "abstracts/piotr-suder.html",
    "title": "Empirical Bound Information-Directed Sampling for Norm-Agnostic Bandits",
    "section": "",
    "text": "Information-directed sampling (IDS) is a powerful framework for solving bandit problems which has shown strong results in both Bayesian and frequentist settings. However, frequentist IDS, like many other bandit algorithms, requires that one have prior knowledge of a (relatively) tight upper bound on the norm of the true parameter vector governing the reward model in order to achieve good performance. Unfortunately, this requirement is rarely satisfied in practice. As we demonstrate, using a poorly calibrated bound can lead to significant regret accumulation. To address this issue, we introduce a novel frequentist IDS algorithm that iteratively refines a high-probability upper bound on the true parameter norm using accumulating data. We focus on the linear bandit setting with heteroskedastic subgaussian noise. Our method leverages a mixture of relevant information gain criteria to balance exploration aimed at tightening the estimated parameter norm bound and directly searching for the optimal action. We establish regret bounds for our algorithm that do not depend on an initially assumed parameter norm bound and demonstrate that our method outperforms state-of-the-art IDS and UCB algorithms.\n\n\nEric Laber\n\n\n\nI am a third year PhD student in the Department of Statistical Science working with Dr. Eric Laber. I am broadly interested in optimization and machine learning, including bandit algorithms, Bayesian optimization, reinforcement learning, and transfer learning. Before coming to Duke I got my Bachelors in Mathematics and Statistics from the University of Florida."
  },
  {
    "objectID": "abstracts/piotr-suder.html#abstract",
    "href": "abstracts/piotr-suder.html#abstract",
    "title": "Empirical Bound Information-Directed Sampling for Norm-Agnostic Bandits",
    "section": "",
    "text": "Information-directed sampling (IDS) is a powerful framework for solving bandit problems which has shown strong results in both Bayesian and frequentist settings. However, frequentist IDS, like many other bandit algorithms, requires that one have prior knowledge of a (relatively) tight upper bound on the norm of the true parameter vector governing the reward model in order to achieve good performance. Unfortunately, this requirement is rarely satisfied in practice. As we demonstrate, using a poorly calibrated bound can lead to significant regret accumulation. To address this issue, we introduce a novel frequentist IDS algorithm that iteratively refines a high-probability upper bound on the true parameter norm using accumulating data. We focus on the linear bandit setting with heteroskedastic subgaussian noise. Our method leverages a mixture of relevant information gain criteria to balance exploration aimed at tightening the estimated parameter norm bound and directly searching for the optimal action. We establish regret bounds for our algorithm that do not depend on an initially assumed parameter norm bound and demonstrate that our method outperforms state-of-the-art IDS and UCB algorithms.\n\n\nEric Laber\n\n\n\nI am a third year PhD student in the Department of Statistical Science working with Dr. Eric Laber. I am broadly interested in optimization and machine learning, including bandit algorithms, Bayesian optimization, reinforcement learning, and transfer learning. Before coming to Duke I got my Bachelors in Mathematics and Statistics from the University of Florida."
  },
  {
    "objectID": "abstracts/yen-chun-liu.html",
    "href": "abstracts/yen-chun-liu.html",
    "title": "Respecting the boundaries: Space-filling designs for surrogate modeling with boundary information",
    "section": "",
    "text": "Gaussian process (GP) surrogate models are widely used for emulating expensive computer simulators, and have led to important advances in science and engineering. One challenge with fitting such surrogates is the costly generation of training data, which can require thousands of CPU hours per run. Recent promising work has investigated the integration of known boundary information within GP surrogates, which can greatly reduce its required training sample size and thus its computational cost. There is, however, little work exploring the critical question of how such simulation experiments should be designed given boundary information. We thus propose here a new class of space-filling designs, called boundary maximin designs, for effective GP surrogate modeling with boundary information. Our designs rely on a new space-filling criterion that is derived from the asymptotic D-optimal designs of the boundary GPs from Vernon et al. (2019) and Ding et al. (2019), which can incorporate a broad class of known boundaries, including axis-parallel and/or perpendicular boundaries. To account for effect sparsity with many variables, we further propose a new boundary maximum projection design that jointly integrates boundary information and ensures good projective properties. Numerical experiments and an application to solving expensive partial differential equations show the improved surrogate performance of boundary-integrated GPs using the proposed boundary maximin designs compared to the state-of-the-art.\n\n\nSimon Mak\n\n\n\nYen-Chun is a 3rd year PhD student interested in experimental designs and sequential decision making."
  },
  {
    "objectID": "abstracts/yen-chun-liu.html#abstract",
    "href": "abstracts/yen-chun-liu.html#abstract",
    "title": "Respecting the boundaries: Space-filling designs for surrogate modeling with boundary information",
    "section": "",
    "text": "Gaussian process (GP) surrogate models are widely used for emulating expensive computer simulators, and have led to important advances in science and engineering. One challenge with fitting such surrogates is the costly generation of training data, which can require thousands of CPU hours per run. Recent promising work has investigated the integration of known boundary information within GP surrogates, which can greatly reduce its required training sample size and thus its computational cost. There is, however, little work exploring the critical question of how such simulation experiments should be designed given boundary information. We thus propose here a new class of space-filling designs, called boundary maximin designs, for effective GP surrogate modeling with boundary information. Our designs rely on a new space-filling criterion that is derived from the asymptotic D-optimal designs of the boundary GPs from Vernon et al. (2019) and Ding et al. (2019), which can incorporate a broad class of known boundaries, including axis-parallel and/or perpendicular boundaries. To account for effect sparsity with many variables, we further propose a new boundary maximum projection design that jointly integrates boundary information and ensures good projective properties. Numerical experiments and an application to solving expensive partial differential equations show the improved surrogate performance of boundary-integrated GPs using the proposed boundary maximin designs compared to the state-of-the-art.\n\n\nSimon Mak\n\n\n\nYen-Chun is a 3rd year PhD student interested in experimental designs and sequential decision making."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Graduate Student Research Seminar Series",
    "section": "",
    "text": "Time: Mondays 11:45 - 1:00 pm\nPlace: Old Chem 116\nInstructor: Merlise Clyde clyde@duke.edu"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Graduate Student Research Seminar Series",
    "section": "Schedule ",
    "text": "Schedule \n\n\n\n\n\n\n\n\nDate\nModerator\nSpeaker\nTitle\n\n\n\n\n13 Jan\nFaculty Meeting\n\n\n\n\n\n\n\n\nNo Talks\n\n\n\n\n\n\n20 Jan\nMLK Holiday\n\n\n\n\n\n\n\n\nNo Talks\n\n\n\n\n\n\n27 Jan\nHun Kang\nCathy Lee\nSharp Bounding Null Effects in Causal Experiments with Ordinal Outcomes\n\n\n\n\n\n\nHoujie Wang\nLikelihood-based Inference on Partially Observed Epidemics and Network Dynamics\n\n\n3 Feb\nFaculty Meeting\n\n\n\n\n\n\n\n\nNo Talks\n\n\n\n\n\n\n10 Feb\nSylvia Vincent\nBonjung Sung\nTesting Separability of Covariance Matrices in High-Dimensional Settings\n\n\n\n\n\n\nKateryna \"Kat\" Husar\nRerandomization with Missing Data\n\n\n17 Feb\nJoshua Lim\nLuke Vrotsos\nDynamic graphical models: Theory, structure and counterfactual forecasting\n\n\n\n\n\n\n\n\n\n\n\n\n24 Feb\nSerim Hong\nBraden Scherting\nSo many bugs… Decoupling NMF\n\n\n\n\n\n\nLorenzo Mauri\nSpectral decomposition-assisted multi-study factor analysis\n\n\n3 Mar\nFaculty Meeting\n\n\n\n\n\n\n\n\nNo Talks\n\n\n\n\n\n\n10 Mar\nSpring Break\n\n\n\n\n\n\n\n\nNo Talks\n\n\n\n\n\n\n17 Mar\nLuigi Malgieri\nAihua Li\nA Bayesian decision-theoretic approach to sparse estimation\n\n\n\n\n\n\nYen-Chun Liu\nRespecting the boundaries: Space-filling designs for surrogate modeling with boundary information\n\n\n24 Mar\nYinyihong Liu\nPiotr Suder\nEmpirical Bound Information-Directed Sampling for Norm-Agnostic Bandits\n\n\n\n\n\n\nBennedetta Bruni\nA partial likelihood approach to tree-based density modeling and its application in Bayesian inference\n\n\n31 Mar\nHan Chen\nSuchismita Roy\nSIR Models under Missing Data: Marginal likelihoods and Dynamical Survival Analysis\n\n\n\n\n\n\nLeah Johnson\nAdversarial Graph Traversal\n\n\n7 Apr\nFaculty Meeting\n\n\n\n\n\n\n\n\nNo Talks\n\n\n\n\n\n\n14 Apr\nRuwimal Pathiraja\nCaitrin Murphy\n\n\n\n\n\n\n\n\nYueqi Guo\n\n\n\n\n21 Apr\nReading Period\nMark Steel"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This course provides graduate students with the opportunity to share their research with other students and faculty, as well as practice giving good research presentations. Each session will consist of either two short presentations or one long presentation. Presenters will receive constructive feedback from both myself and the audience, on both the presentation and research content. Students are encouraged (and expected) to contribute to discussions - this helps both the presenter and your own research!"
  },
  {
    "objectID": "about.html#description",
    "href": "about.html#description",
    "title": "About",
    "section": "",
    "text": "This course provides graduate students with the opportunity to share their research with other students and faculty, as well as practice giving good research presentations. Each session will consist of either two short presentations or one long presentation. Presenters will receive constructive feedback from both myself and the audience, on both the presentation and research content. Students are encouraged (and expected) to contribute to discussions - this helps both the presenter and your own research!"
  },
  {
    "objectID": "about.html#logistics",
    "href": "about.html#logistics",
    "title": "About",
    "section": "Logistics",
    "text": "Logistics\n\nSpeakers are expected to present in-person. A Zoom link will be available only for students who cannot physically attend or external audience members.\nStudents presenting will invite their PhD advisors and committee members to attend.\nAll PhD students in Statistical Science are recommended to register for STA 701S each semester, and all PhD students in Statistical Science in Years 3+ of studies will be required to register and present in STA 701S each year.\nThere will be two talks scheduled in each class. Each talk should be no longer than 25 minutes, which leaves ample time for questions & suggestions from the audience.\nFirst and second year PhD students will serve as Moderators, who will introduce the speakers and facilitate Q&A.\nAny students who will need to change the date of the presentation should submit a request in Github.1 Please provide the alternate date(s) and confirmation that the other individual is willing to switch.\nPlease submit titles and abstracts at least one week in advance."
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "About",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou must be logged in to GitHub to submit a request.↩︎"
  },
  {
    "objectID": "abstracts/leah-johnson.html",
    "href": "abstracts/leah-johnson.html",
    "title": "Adversarial Graph Traversal",
    "section": "",
    "text": "Real‑world sequential decision problems—from career planning to supply‑chain routing—can often be modeled from the point of view of an agent traversing a known network, with unknown opportunities and costs. In the non‑adversarial setting, Caballero et al. (2025) study Bayesian graph traversal under a Gaussian process prior for node reward and edge cost functions, proposing several heuristics that aim to maximize expected payoff. However, many practical environments also include adversarial actors who can impose additional costs on a traveler’s route or reduce available rewards.\nWe present ongoing work on an application of Adversarial Risk Analysis (ARA) to graph navigation, which departs from traditional game‑theoretic formulations by allowing the decision‑maker to maintain her own probabilistic beliefs about both environmental uncertainty and adversarial behavior—without requiring common knowledge or rationality assumptions. We illustrate this approach through a simple example in which an adversary, fully informed of the graph structure and underlying payoffs, can impose a fixed penalty on any node adjacent to the traveler’s current position.\n\n\nEric Laber, David Banks\n\n\n\nLeah is a 3rd year PhD student interested in sequential decision making, and clinical trial design."
  },
  {
    "objectID": "abstracts/leah-johnson.html#abstract",
    "href": "abstracts/leah-johnson.html#abstract",
    "title": "Adversarial Graph Traversal",
    "section": "",
    "text": "Real‑world sequential decision problems—from career planning to supply‑chain routing—can often be modeled from the point of view of an agent traversing a known network, with unknown opportunities and costs. In the non‑adversarial setting, Caballero et al. (2025) study Bayesian graph traversal under a Gaussian process prior for node reward and edge cost functions, proposing several heuristics that aim to maximize expected payoff. However, many practical environments also include adversarial actors who can impose additional costs on a traveler’s route or reduce available rewards.\nWe present ongoing work on an application of Adversarial Risk Analysis (ARA) to graph navigation, which departs from traditional game‑theoretic formulations by allowing the decision‑maker to maintain her own probabilistic beliefs about both environmental uncertainty and adversarial behavior—without requiring common knowledge or rationality assumptions. We illustrate this approach through a simple example in which an adversary, fully informed of the graph structure and underlying payoffs, can impose a fixed penalty on any node adjacent to the traveler’s current position.\n\n\nEric Laber, David Banks\n\n\n\nLeah is a 3rd year PhD student interested in sequential decision making, and clinical trial design."
  },
  {
    "objectID": "abstracts/suchismita-roy.html",
    "href": "abstracts/suchismita-roy.html",
    "title": "SIR Models under Missing Data: Marginal likelihoods and Dynamical Survival Analysis",
    "section": "",
    "text": "The SIR model is a compartmental model widely used to model epidemic dynamics. Despite its widespread use, inferring parameters from such compartmental models using partially observed data presents significant challenges due to the intractability of the likelihood. To address this, we develop a closed-form approximate likelihood using the Dynamical Survival Analysis (DSA) method, which offers flexibility and computational efficiency. Through a simulation study, we assess its performance and compare it with the PDSIR method, which is also designed for inference using incidence data. To further demonstrate the adaptability of our approach, we extend the likelihood to frailty models, illustrating how it can be modified to incorporate individual heterogeneity. Finally, we apply our method to real-world data from the 2018–2020 Ebola outbreak in the Democratic Republic of the Congo, demonstrating its practical utility for epidemic inference with limited observations.\n\n\nDr. Jason Xu and Dr. Alexander Fisher\n\n\n\na brief bio - I am a third-year PhD student. I am interested in epidemic models and Bayesian phylogenetics."
  },
  {
    "objectID": "abstracts/suchismita-roy.html#abstract",
    "href": "abstracts/suchismita-roy.html#abstract",
    "title": "SIR Models under Missing Data: Marginal likelihoods and Dynamical Survival Analysis",
    "section": "",
    "text": "The SIR model is a compartmental model widely used to model epidemic dynamics. Despite its widespread use, inferring parameters from such compartmental models using partially observed data presents significant challenges due to the intractability of the likelihood. To address this, we develop a closed-form approximate likelihood using the Dynamical Survival Analysis (DSA) method, which offers flexibility and computational efficiency. Through a simulation study, we assess its performance and compare it with the PDSIR method, which is also designed for inference using incidence data. To further demonstrate the adaptability of our approach, we extend the likelihood to frailty models, illustrating how it can be modified to incorporate individual heterogeneity. Finally, we apply our method to real-world data from the 2018–2020 Ebola outbreak in the Democratic Republic of the Congo, demonstrating its practical utility for epidemic inference with limited observations.\n\n\nDr. Jason Xu and Dr. Alexander Fisher\n\n\n\na brief bio - I am a third-year PhD student. I am interested in epidemic models and Bayesian phylogenetics."
  },
  {
    "objectID": "abstracts/aihua-li.html",
    "href": "abstracts/aihua-li.html",
    "title": "A Bayesian decision-theoretic approach to sparse estimation",
    "section": "",
    "text": "We extend the work of Hahn & Carvalho (2015) and develop a doubly-regularized sparse regression estimator by synthesizing Bayesian regularization with penalized least squares within a decision-theoretic framework. In contrast to existing Bayesian decision-theoretic formulation chiefly reliant upon the symmetric 0-1 loss, the new method – which we call Bayesian Decoupling – employs a family of penalized loss functions indexed by a sparsity-tuning parameter. We propose a class of reweighted l1 penalties, with two specific instances that achieve simultaneous bias reduction and convexity. The design of the penalties incorporates considerations of signal sizes, as enabled by the Bayesian paradigm. The tuning parameter is selected using a posterior benchmarking criterion, which quantifies the drop in predictive power relative to the posterior mean which is the optimal Bayes estimator under the squared error loss. Additionally, in contrast to the widely used median probability model technique which selects variables by thresholding posterior inclusion probabilities at the fixed threshold of 1/2, Bayesian Decoupling enables the use of a data-driven threshold which automatically adapts to estimated signal sizes and offers far better performance in high-dimensional settings with highly correlated predictors. Our numerical results in such settings show that certain combinations of priors and loss functions significantly improve the solution path compared to existing methods, prioritizing true signals early along the path before false signals are selected. Consequently, Bayesian Decoupling produces estimates with better prediction and selection performance. Finally, a real data application illustrates the practical advantages of our approaches which select sparser models with larger coefficient estimates.\n\n\nSurya Tokdar, Jason Xu\n\n\n\n3rd year phd student"
  },
  {
    "objectID": "abstracts/aihua-li.html#abstract",
    "href": "abstracts/aihua-li.html#abstract",
    "title": "A Bayesian decision-theoretic approach to sparse estimation",
    "section": "",
    "text": "We extend the work of Hahn & Carvalho (2015) and develop a doubly-regularized sparse regression estimator by synthesizing Bayesian regularization with penalized least squares within a decision-theoretic framework. In contrast to existing Bayesian decision-theoretic formulation chiefly reliant upon the symmetric 0-1 loss, the new method – which we call Bayesian Decoupling – employs a family of penalized loss functions indexed by a sparsity-tuning parameter. We propose a class of reweighted l1 penalties, with two specific instances that achieve simultaneous bias reduction and convexity. The design of the penalties incorporates considerations of signal sizes, as enabled by the Bayesian paradigm. The tuning parameter is selected using a posterior benchmarking criterion, which quantifies the drop in predictive power relative to the posterior mean which is the optimal Bayes estimator under the squared error loss. Additionally, in contrast to the widely used median probability model technique which selects variables by thresholding posterior inclusion probabilities at the fixed threshold of 1/2, Bayesian Decoupling enables the use of a data-driven threshold which automatically adapts to estimated signal sizes and offers far better performance in high-dimensional settings with highly correlated predictors. Our numerical results in such settings show that certain combinations of priors and loss functions significantly improve the solution path compared to existing methods, prioritizing true signals early along the path before false signals are selected. Consequently, Bayesian Decoupling produces estimates with better prediction and selection performance. Finally, a real data application illustrates the practical advantages of our approaches which select sparser models with larger coefficient estimates.\n\n\nSurya Tokdar, Jason Xu\n\n\n\n3rd year phd student"
  },
  {
    "objectID": "abstracts/kat_huzar.html",
    "href": "abstracts/kat_huzar.html",
    "title": "Rerandomization with missing data",
    "section": "",
    "text": "Randomized control trials are considered the gold standard in research because they allow for high confidence in establishing cause-and-effect relationships. Randomly assigning participants to the treatment or control group ensures that any observed differences in outcomes between these groups can be attributed to the intervention rather than external factors. However, differences between the groups can still occur due to chance, potentially resulting in misleading results. The issue of imbalance on observed covariates can be addressed in the design phase: rerandomization selects a treatment assignment from a subset of assignments that satisfy a predetermined balance criterion for pre-treatment covariates. Under rerandomization, classical estimators yield a more precise estimator and combining rerandomization with the regression adjustment can further improve inference. In practice, even in the pre-treatment stage, there may be substantial missingness in the data, which in turn can reduce the improvements due to rerandomization which cannot be addressed by simple post-hoc regression adjustment. By introducing missing data imputation methods into the rerandomization, we are able to recover the efficiency losses for estimating average treatment effects. We also show how performing rerandomization that adjusts for missingness combined with regression adjustment increases the precision of the estimates compared to regression adjustment alone, and recommends the use of rerandomization in the design of the study when missing data are present.\n\n\nMy advisor is Alex Volfovsky\n\n\n\nKat is a third year student interested in designing experiments that improve covariate balance and better the efficiency of causal estimates."
  },
  {
    "objectID": "abstracts/kat_huzar.html#abstract",
    "href": "abstracts/kat_huzar.html#abstract",
    "title": "Rerandomization with missing data",
    "section": "",
    "text": "Randomized control trials are considered the gold standard in research because they allow for high confidence in establishing cause-and-effect relationships. Randomly assigning participants to the treatment or control group ensures that any observed differences in outcomes between these groups can be attributed to the intervention rather than external factors. However, differences between the groups can still occur due to chance, potentially resulting in misleading results. The issue of imbalance on observed covariates can be addressed in the design phase: rerandomization selects a treatment assignment from a subset of assignments that satisfy a predetermined balance criterion for pre-treatment covariates. Under rerandomization, classical estimators yield a more precise estimator and combining rerandomization with the regression adjustment can further improve inference. In practice, even in the pre-treatment stage, there may be substantial missingness in the data, which in turn can reduce the improvements due to rerandomization which cannot be addressed by simple post-hoc regression adjustment. By introducing missing data imputation methods into the rerandomization, we are able to recover the efficiency losses for estimating average treatment effects. We also show how performing rerandomization that adjusts for missingness combined with regression adjustment increases the precision of the estimates compared to regression adjustment alone, and recommends the use of rerandomization in the design of the study when missing data are present.\n\n\nMy advisor is Alex Volfovsky\n\n\n\nKat is a third year student interested in designing experiments that improve covariate balance and better the efficiency of causal estimates."
  },
  {
    "objectID": "abstracts/Houjie_Wang.html",
    "href": "abstracts/Houjie_Wang.html",
    "title": "Likelihood-based Inference on Partially Observed Epidemics and Network Dynamics",
    "section": "",
    "text": "Pandemic modeling based on dynamic social contact network has gradually gained popularity and has been proved to be better at parameter estimation than the traditional epidemic models based on the traditional``random mixing assumption’’. However, leveraging the dynamics of the contact network requires very high-resolution data, which is costly to collect and could raise privacy concerns. To relaxed the need for granular observations, we propose a data-augmentation method for an advanced individual-level framework with interplay between the SIR-type epidemics and an underlying dynamic social contact network, which allows the social contact network to be observed at a very low frequency. By repeated simulation of disease transmission in a dynamic social contact network, we show that our method with the coarsened data is able to carry out valid estimation and inference of the infection rate of the pandemic. We applied our method to an actual dataset of on-campus university students suffered from a flu pandemic where granular observations of their epidemic status and dynamic contact network is available, and the result shows that the estimation with the coarsened data is close to the granular data MLE. This verifies the method and shreds light on its application to a larger scale.\n\n\nAlexander Volfovsky\n\n\n\nBS in statistics at Texas A&M University; MS in in statistics at University of Washington. Currently 3rd year in the department. Interested to work on methodologies to address applied modeling problems in network and time series data."
  },
  {
    "objectID": "abstracts/Houjie_Wang.html#abstract",
    "href": "abstracts/Houjie_Wang.html#abstract",
    "title": "Likelihood-based Inference on Partially Observed Epidemics and Network Dynamics",
    "section": "",
    "text": "Pandemic modeling based on dynamic social contact network has gradually gained popularity and has been proved to be better at parameter estimation than the traditional epidemic models based on the traditional``random mixing assumption’’. However, leveraging the dynamics of the contact network requires very high-resolution data, which is costly to collect and could raise privacy concerns. To relaxed the need for granular observations, we propose a data-augmentation method for an advanced individual-level framework with interplay between the SIR-type epidemics and an underlying dynamic social contact network, which allows the social contact network to be observed at a very low frequency. By repeated simulation of disease transmission in a dynamic social contact network, we show that our method with the coarsened data is able to carry out valid estimation and inference of the infection rate of the pandemic. We applied our method to an actual dataset of on-campus university students suffered from a flu pandemic where granular observations of their epidemic status and dynamic contact network is available, and the result shows that the estimation with the coarsened data is close to the granular data MLE. This verifies the method and shreds light on its application to a larger scale.\n\n\nAlexander Volfovsky\n\n\n\nBS in statistics at Texas A&M University; MS in in statistics at University of Washington. Currently 3rd year in the department. Interested to work on methodologies to address applied modeling problems in network and time series data."
  },
  {
    "objectID": "abstracts/TBA.html#abstract",
    "href": "abstracts/TBA.html#abstract",
    "title": "701 abstract",
    "section": "Abstract",
    "text": "Abstract\nTBA\n\n\nAdvisor(s)\nProf.\n\n\nBio"
  },
  {
    "objectID": "abstracts/houjie_wang.html",
    "href": "abstracts/houjie_wang.html",
    "title": "Likelihood-based Inference on Partially Observed Epidemics and Network Dynamics",
    "section": "",
    "text": "Pandemic modeling based on dynamic social contact network has gradually gained popularity and has been proved to be better at parameter estimation than the traditional epidemic models based on the traditional``random mixing assumption’’. However, leveraging the dynamics of the contact network requires very high-resolution data, which is costly to collect and could raise privacy concerns. To relaxed the need for granular observations, we propose a data-augmentation method for an advanced individual-level framework with interplay between the SIR-type epidemics and an underlying dynamic social contact network, which allows the social contact network to be observed at a very low frequency. By repeated simulation of disease transmission in a dynamic social contact network, we show that our method with the coarsened data is able to carry out valid estimation and inference of the infection rate of the pandemic. We applied our method to an actual dataset of on-campus university students suffered from a flu pandemic where granular observations of their epidemic status and dynamic contact network is available, and the result shows that the estimation with the coarsened data is close to the granular data MLE. This verifies the method and shreds light on its application to a larger scale.\n\n\nAlexander Volfovsky\n\n\n\nBS in statistics at Texas A&M University; MS in in statistics at University of Washington. Currently 3rd year in the department. Interested to work on methodologies to address applied modeling problems in network and time series data."
  },
  {
    "objectID": "abstracts/houjie_wang.html#abstract",
    "href": "abstracts/houjie_wang.html#abstract",
    "title": "Likelihood-based Inference on Partially Observed Epidemics and Network Dynamics",
    "section": "",
    "text": "Pandemic modeling based on dynamic social contact network has gradually gained popularity and has been proved to be better at parameter estimation than the traditional epidemic models based on the traditional``random mixing assumption’’. However, leveraging the dynamics of the contact network requires very high-resolution data, which is costly to collect and could raise privacy concerns. To relaxed the need for granular observations, we propose a data-augmentation method for an advanced individual-level framework with interplay between the SIR-type epidemics and an underlying dynamic social contact network, which allows the social contact network to be observed at a very low frequency. By repeated simulation of disease transmission in a dynamic social contact network, we show that our method with the coarsened data is able to carry out valid estimation and inference of the infection rate of the pandemic. We applied our method to an actual dataset of on-campus university students suffered from a flu pandemic where granular observations of their epidemic status and dynamic contact network is available, and the result shows that the estimation with the coarsened data is close to the granular data MLE. This verifies the method and shreds light on its application to a larger scale.\n\n\nAlexander Volfovsky\n\n\n\nBS in statistics at Texas A&M University; MS in in statistics at University of Washington. Currently 3rd year in the department. Interested to work on methodologies to address applied modeling problems in network and time series data."
  }
]